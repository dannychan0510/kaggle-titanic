size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3, colour = "black", aes(y = ..density..))
g <- g + stat_function(fun = dnorm, size = 2)
g + facet_grid(. ~ size)
nosim <- 1000
cfunc <- function(x, n) sqrt(n) * (mean(x) - 3.5) / 1.71
dat <- data.frame(
x = c(apply(matrix(sample(1 : 6, nosim * 10, replace = TRUE),
nosim), 1, cfunc, 10),
apply(matrix(sample(1 : 6, nosim * 20, replace = TRUE),
nosim), 1, cfunc, 20),
apply(matrix(sample(1 : 6, nosim * 30, replace = TRUE),
nosim), 1, cfunc, 30)
),
size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.3, colour = "black", aes(y = ..density..))
g <- g + stat_function(fun = dnorm, size = 2)
g + facet_grid(. ~ size)
View(dat)
nosim <- 1000
cfunc <- function(x, n) sqrt(n) * (mean(x) - 3.5) / 1.71
dat <- data.frame(
x = c(apply(matrix(sample(1 : 6, nosim * 10, replace = TRUE),
nosim), 1, cfunc, 10),
),
size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.3, colour = "black", aes(y = ..density..))
g <- g + stat_function(fun = dnorm, size = 2)
g + facet_grid(. ~ size)
nosim <- 1000
cfunc <- function(x, n) 2 * sqrt(n) * (mean(x) - 0.5)
dat <- data.frame(
x = c(apply(matrix(sample(0:1, nosim * 10, replace = TRUE),
nosim), 1, cfunc, 10),
apply(matrix(sample(0:1, nosim * 20, replace = TRUE),
nosim), 1, cfunc, 20),
apply(matrix(sample(0:1, nosim * 30, replace = TRUE),
nosim), 1, cfunc, 30)
),
size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
View(dat)
size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
View(dat)
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3, colour = "black", aes(y = ..density..))
g <- g + stat_function(fun = dnorm, size = 2)
g + facet_grid(. ~ size)
mns = NULL
for (i in 1 : 1000) mns = c(mns, mean(runif(40)))
mns
hist(mns)
mns = NULL
for (i in 1 : 1000) mns = c(mns, sd(runif(40)))
hist(mns)
data(sleep)
sleep <- sleep
View(sleep)
1100 + c(-1, 1) * qt(0.975, 9 - 1) * 30
1100 + c(-1, 1) * qt(0.95, 9 - 1) * 30
1100 + c(-1, 1) * qt(0.95 / 2, 9 - 1) * 30
1100 + c(-1, 1) * qt(0.95 / 2, 9 - 1) * 30 / (9)^.4
1100 + c(-1, 1) * qt(0.95 / 2, 9 - 1) * 30 / (9)^.5
help qt
?qt
xbar <- 1100
sd <- 30
n <- 9
mean + c(-1, 1) * qt(0.95, n-1) * (sd / sqrt(n))
xbar <- 1100
sd <- 30
n <- 9
xbar + c(-1, 1) * qt(0.95, n-1) * (sd / sqrt(n))
xbar <- 1100
sd <- 30
n <- 9
xbar + c(-1, 1) * qt(0.975, n-1) * (sd / sqrt(n))
qt(0.975, 8)
xbar <- -2
n <- 9
xbar * sqrt(n) / qt(0.975, n-1)
xbar_old <- 3
sd_old <- 0.6
n_old <- 10
xbar_new <- 5
sd_new <- 0.68
n_new <- 10
pooled_var <- (((n_old - 1)*sd_old^2) + ((n_new - 1)*sd_new^2)) / (n_old + n_new - 2)
(n_new - n_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_old <- 3
sd_old <- 0.6
n_old <- 10
xbar_new <- 5
sd_new <- 0.68
n_new <- 20
pooled_var <- (((n_old - 1)*sd_old^2) + ((n_new - 1)*sd_new^2)) / (n_old + n_new - 2)
(n_new - n_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_old <- 5
sd_old <- 0.68
n_old <- 10
xbar_new <- 3
sd_new <- 0.6
n_new <- 20
pooled_var <- (((n_old - 1)*sd_old^2) + ((n_new - 1)*sd_new^2)) / (n_old + n_new - 2)
(n_new - n_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
(n_new - n_old) + c(-1, 1) * qt(0.975 / 2, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_old <- 5
sd_old <- 0.68
n_old <- 10
xbar_new <- 3
sd_new <- 0.6
n_new <- 20
pooled_var <- (((n_old - 1)*sd_old^2) + ((n_new - 1)*sd_new^2)) / (n_old + n_new - 2)
(n_new - n_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_old <- 5
var_old <- 0.68
n_old <- 10
xbar_new <- 3
var_new <- 0.6
n_new <- 20
pooled_var <- (((n_old - 1)*sd_old) + ((n_new - 1)*sd_new)) / (n_old + n_new - 2)
(n_new - n_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_old <- 5
var_old <- 0.68
n_old <- 10
xbar_new <- 3
var_new <- 0.6
n_new <- 20
pooled_var <- (((n_old - 1)*var_old) + ((n_new - 1)*var_new)) / (n_old + n_new - 2)
(n_new - n_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_old <- 5
var_old <- 0.68
n_old <- 10
xbar_new <- 3
var_new <- 0.6
n_new <- 20
pooled_var <- (((n_old - 1)*var_old) + ((n_new - 1)*var_new)) / (n_old + n_new - 2)
(n_new - n_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_old <- 5
var_old <- 0.68
n_old <- 10
xbar_new <- 3
var_new <- 0.6
n_new <- 10
pooled_var <- (((n_old - 1)*var_old) + ((n_new - 1)*var_new)) / (n_old + n_new - 2)
(n_new - n_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_old <- 5
var_old <- 0.68
n_old <- 10
xbar_new <- 3
var_new <- 0.6
n_new <- 10
pooled_var <- (((n_old - 1)*var_old) + ((n_new - 1)*var_new)) / (n_old + n_new - 2)
(n_new - n_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_old <- 5
var_old <- 0.68
n_old <- 10
xbar_new <- 3
var_new <- 0.6
n_new <- 10
pooled_var <- (((n_old - 1)*var_old) + ((n_new - 1)*var_new)) / (n_old + n_new - 2)
(n_new - n_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) / sqrt((n_old) + (n_new))
xbar_old <- 5
var_old <- 0.68
n_old <- 10
xbar_new <- 3
var_new <- 0.6
n_new <- 10
pooled_var <- (((n_old - 1)*var_old) + ((n_new - 1)*var_new)) / (n_old + n_new - 2)
(n_new - n_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
(xbar_new - xbar_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_old <- 5
var_old <- 0.68
n_old <- 10
xbar_new <- 3
var_new <- 0.6
n_new <- 10
pooled_var <- (((n_old - 1)*var_old) + ((n_new - 1)*var_new)) / (n_old + n_new - 2)
(xbar_new - xbar_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
(xbar_new - xbar_old) + c(-1, 1) * qt(0.95, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_old <- 6
var_old <- 2
n_old <- 100
xbar_new <- 4
var_new <- 0.5
n_new <- 100
pooled_var <- (((n_old - 1)*var_old) + ((n_new - 1)*var_new)) / (n_old + n_new - 2)
(xbar_new - xbar_old) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_old <- 6
var_old <- 2
n_old <- 100
xbar_new <- 4
var_new <- 0.5
n_new <- 100
pooled_var <- (((n_old - 1)*var_old) + ((n_new - 1)*var_new)) / (n_old + n_new - 2)
(xbar_old - xbar_new) + c(-1, 1) * qt(0.975, n_old + n_new - 2) * sqrt(pooled_var) * sqrt((1 / n_old) + (1 / n_new))
xbar_placebo <- 1
var_placebo <- 1.8^2
n_placebo <- 9
xbar_treatment <- -3
var_treatment <- 1.5^2
n_treatment <- 9
pooled_var <- (((n_placebo - 1)*var_placebo) + ((n_treatment - 1)*var_treatment)) / (n_placebo + n_treatment - 2)
(xbar_placebo - xbar_treatment) + c(-1, 1) * qt(0.975, n_placebo + n_treatment - 2) * sqrt(pooled_var) * sqrt((1 / n_placebo) + (1 / n_treatment))
xbar_placebo <- 1
var_placebo <- 1.8^2
n_placebo <- 9
xbar_treatment <- -3
var_treatment <- 1.5^2
n_treatment <- 9
pooled_var <- (((n_placebo - 1)*var_placebo) + ((n_treatment - 1)*var_treatment)) / (n_placebo + n_treatment - 2)
(xbar_placebo - xbar_treatment) + c(-1, 1) * qt(0.95, n_placebo + n_treatment - 2) * sqrt(pooled_var) * sqrt((1 / n_placebo) + (1 / n_treatment))
xbar_placebo <- 1
var_placebo <- 1.8^2
n_placebo <- 9
xbar_treatment <- -3
var_treatment <- 1.5^2
n_treatment <- 9
pooled_var <- (((n_placebo - 1)*var_placebo) + ((n_treatment - 1)*var_treatment)) / (n_placebo + n_treatment - 2)
(xbar_treatment - xbar_placebo) + c(-1, 1) * qt(0.95, n_placebo + n_treatment - 2) * sqrt(pooled_var) * sqrt((1 / n_placebo) + (1 / n_treatment))
?power.t.test
week1 <- c(140, 138, 150, 148, 135)
week2 <- c(132, 135, 151, 146, 130)
t.test(week1, week2, alternative = "two.sided", paired = T)
week1 <- c(140, 138, 150, 148, 135)
week2 <- c(132, 135, 151, 146, 130)
t.test(week1, week2, alternative = "two.sided", paired = T)
m0 <- 1081
power.t.test(n = 9, sd = 30, delta = m0 / 1100, type = "one.sample", alt = "one.sided")
m0 <- 1119
power.t.test(n = 9, sd = 30, delta = m0 / 1100, type = "one.sample", alt = "one.sided")
m0 <- 1031
power.t.test(n = 9, sd = 30, delta = m0 / 1100, type = "one.sample", alt = "one.sided")
m0 <- 1080
power.t.test(n = 9, sd = 30, delta = m0 / 1100, type = "one.sample", alt = "one.sided")
m0 <- 1077
power.t.test(n = 9, sd = 30, delta = m0 / 1100, type = "one.sample", alt = "one.sided")
n = 9; mu = 1100; sd=30
mu + c(-1,1) * qt(.975, n-1) * sd/sqrt(n)
n <- 9
m0 <- 1100
sd <- 30
m0 + c(-1, 1) * qt(.975, n - 1) * sd / sqrt(n)
binom.test(3, 4, alt= "greater")$p.value
?poisson.test
poisson.test(x = 10, T = 1787, r = 1/100, alternative = "less")$p.value
poisson.test(x = 10, T = 1787, r = 1/100, alternative = "less")
n <- 100 #subject
μ <- 0.01# m^3 brain volume loss mean
σ <- 0.04# m^3 brain volume loss std. dev.
p <- 0.05 # sign level
pow <- power.t.test(n=n, delta=μ, sd=σ , sig.level=p, type="one.sample", alt="one.sided")$power
round(pow, 2)
μ <- 0.01# m^3 brain volume loss mean
σ <- 0.04# m^3 brain volume loss std. dev.
p <- 0.05 # sign level
pow <- 0.9 #power
n <- power.t.test(power=pow, delta=μ, sd=σ , sig.level=p, type="one.sample", alt="one.sided")$n
ceiling(n/10)*10
library(MASS)
?shuttle
data(shuttle)
str(shuttle)
names(shuttle)
?glm
fit <- glm(use ~ wind, family='binomial', shuttle)
exp(fit$coeff)
fit <- glm(use ~ wind + as.factor(magn), family='binomial', shuttle)
exp(fit$coeff)
library(MASS)
head(shuttle)
shuttle2<-shuttle
shuttle2$use2<-as.numeric(shuttle2$use=='auto')
#shuttle2$wind2<-as.numeric(shuttle2$wind=='head')
#head(shuttle2)
fit<-glm(use2 ~ factor(wind) - 1, family = binomial, data = shuttle2)
#0.03181
#fit<-glm(use ~ wind, family = binomial, data = shuttle)
#anova(fit)
summary(fit)$coef
fit<-glm(use2 ~ factor(wind) + factor(magn) - 1, family = binomial, data = shuttle2)
summary(fit)$coef
exp(coef(fit))
exp(cbind(OddsRatio = coef(fit), confint(fit)))
1.286 / 1.327
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
library(caret)
data <- AlzheimerDisease
install.packages("caret")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
library(caret)
data("AlzheimerDisease")
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data("AlzheimerDisease")
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
library(caret)
data("AlzheimerDisease")
library(ggplot)
library(ggplot2)
# Titanic: Machine Learning from Disaster
# Following http://trevorstephens.com/post/72916401642/titanic-getting-started-with-r
# Project set-up ----------------------------------------------------------
# Setting working directory
setwd("C:\\Users\\895284\\Documents\\GitHub\\kaggle-titanic")
setwd("~/Documents/GitHub/kaggle-titanic")
# Loading libraries
library(ggplot2)
# Import data
genderclassmodel <- read.csv("Data\\genderclassmodel.csv", stringsAsFactors = FALSE)
gendermodel <- read.csv("Data\\gendermodel.csv", stringsAsFactors = FALSE)
test <- read.csv("Data\\test.csv")
train <- read.csv("Data\\train.csv")
genderclassmodel <- read.csv("Data/genderclassmodel.csv", stringsAsFactors = FALSE)
gendermodel <- read.csv("Data/gendermodel.csv", stringsAsFactors = FALSE)
test <- read.csv("Data/test.csv")
train <- read.csv("Data/train.csv")
# All passengers perish ---------------------------------------------------
# Understanding the structure of the data
str(train)
# Looking at the data to see how many people in the training dataset has survived
table(train$Survived)
prop.table(table(train$Survived))
# Create a 'Survived' column in the test dataset where everyone does not survive
test$Survived <- rep(0, 418)
# Create first submission file assuming all passengers perish (score = 0.6269)
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "Submissions/firstsubmission.csv", row.names = FALSE)
# Gender-class model ------------------------------------------------------
# Looking at the split between genders
summary(train$Sex)
# Looking at the proportion of males and females that survived
prop.table(table(train$Sex, train$Survived), 1) # Most females survive (0.74), and most males perish (0.81)
# Create submission where all females survive and all males perish
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
# Create submission where all females survive and all males perish (score = 0.7655)
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "Submissions/gender.csv", row.names = FALSE)
# Looking and plotting at the age variable
summary(train$Age)
qplot(Age, data = train, geom = "histogram", binwidth = 5)
# Creating a child variable, where age is less than 18
train$Child <- 0
train$Child[train$Age < 18] <- 1
# Creating a table with both gender and child status to see who survived
aggregate(Survived ~ Child + Sex, data = train, FUN = sum) # Number of survivors
aggregate(Survived ~ Child + Sex, data = train, FUN = length) # Totals for each category
aggregate(Survived ~ Child + Sex, data = train, FUN = function(x) {sum(x) / length(x)}) # Proportions for each category - does not appear to have significant difference regardless whether person was a child or not. Still large majority of passengers survived if they were a female.
# Create a factor variable that bins the fare variable
train$Fare2 <- '30+'
train$Fare2[train$Fare < 30 & train$Fare >= 20] <- '20-30'
train$Fare2[train$Fare < 20 & train$Fare >= 10] <- '10-20'
train$Fare2[train$Fare < 10] <- '<10'
# Create a table with class, ticket price,  gender and child status to see who survived
aggregate(Survived ~ Fare2 + Pclass + Sex, data = train, FUN = function(x) {sum(x) / length(x)}) # Females in third class paying for more than $20 for a ticket has a lower chance of survival.
# Create submission where all females except for those in third class paying more than $20 for a ticket survive, and the rest perish (score = 0.7799)
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
test$Survived[test$Sex == 'female' & test$Pclass == 3 & test$Fare >= 20] <- 0
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "Submissions/genderclass.csv", row.names = FALSE)
# Decision trees ----------------------------------------------------------
genderclassmodel <- read.csv("Data\\genderclassmodel.csv", stringsAsFactors = FALSE)
gendermodel <- read.csv("Data\\gendermodel.csv", stringsAsFactors = FALSE)
test <- read.csv("Data\\test.csv")
train <- read.csv("Data\\train.csv")
genderclassmodel <- read.csv("Data/genderclassmodel.csv", stringsAsFactors = FALSE)
gendermodel <- read.csv("Data/gendermodel.csv", stringsAsFactors = FALSE)
test <- read.csv("Data/test.csv")
train <- read.csv("Data/train.csv")
# Installing relevant packages
install.packages("rpart")
install.packages('rattle')
install.packages('rpart.plot')
install.packages('RColorBrewer')
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
# Using rpart for the first time!
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train, method="class")
# Plotting the decision tree
fancyRpartPlot(fit)
# Generating submission for Kaggle using this decision tree (score = 0.78469)
Prediction <- predict(fit, test, type = "class")
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "Submissions/decisiontree.csv", row.names = FALSE)
# Feature Engineering -----------------------------------------------------
# Reloading in data to start from blank slate
genderclassmodel <- read.csv("Data\\genderclassmodel.csv", stringsAsFactors = FALSE)
gendermodel <- read.csv("Data\\gendermodel.csv", stringsAsFactors = FALSE)
test <- read.csv("Data\\test.csv")
train <- read.csv("Data\\train.csv")
genderclassmodel <- read.csv("Data/genderclassmodel.csv", stringsAsFactors = FALSE)
gendermodel <- read.csv("Data/gendermodel.csv", stringsAsFactors = FALSE)
test <- read.csv("Data/test.csv")
train <- read.csv("Data/train.csv")
# Creating a combined dataset with both training and test data
test$Survived <- NA
combi <- rbind(train, test)
# Changing name back to character
combi$Name <- as.character(combi$Name)
# Creating a Title variable from the Name variable
combi$Title <- sapply(combi$Name, FUN = function(x) {strsplit(x, split = '[,.]')[[1]][2]})
combi$TItle <- sub(' ', '', combi$Title)
# Combining a few of the unusual titles
combi$Title[(combi$Title %in% c('Mme', 'Mlle'))] <- 'Mlle'
combi$Title[(combi$Title %in% c('Capt', 'Don', 'Major', 'Sir'))] <- 'Sir'
combi$Title[(combi$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer'))] <- 'Lady'
# Changing the Title variable back into a factor variable
combi$Title <- factor(combi$Title)
# Creating a variable which captures the size of the family
combi$FamilySize <- combi$SibSp + combi$Parch + 1
# Creating a Surname variable
combi$Surname <- sapply(combi$Name, FUN = function(x) {strsplit(x, split = '[,.]')[[1]][1]})
# Combining the FamilySize and Surname varaiables
combi$FamilyID <- paste(as.character(combi$FamilySize), combi$Surname, sep = "")
# Identifying families with small sizes
combi$FamilyID[combi$FamilySize <= 2] <- 'Small'
# Data cleaning - some small families slipped through the net
famIDs <- data.frame(table(combi$FamilyID))
famIDs <- famIDs[famIDs$Freq <= 2, ]
combi$FamilyID[combi$FamilyID %in% famIDs$Var1] <- 'Small'
combi$FamilyID <- factor(combi$FamilyID)
# Resplitting the training and test datasets
train <- combi[1:891, ]
test <- combi[892:1309, ]
# Fitting a new prediction tree using feature engineered variables
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID,
data=train, method="class")
# Generating a prediction and submission file for Kaggle
Prediction <- predict(fit, test, type = "class")
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "Submissions/featureengineering.csv", row.names = FALSE)
# Random Forest -----------------------------------------------------------
# Growing a tree for Age
Agefit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + FamilySize,
data=combi[!is.na(combi$Age),], method="anova")
combi$Age[is.na(combi$Age)] <- predict(Agefit, combi[is.na(combi$Age),])
# Fixing the Embared and Fare varibles with missing values
combi$Embarked[which(combi$Embarked == '')] = "S"
combi$Fare[which(is.na(combi$Fare))] <- median(combi$Fare, na.rm=TRUE)
# Generating a new FamilyID variable because the random forest function in R cannot take in too many factors
combi$FamilyID2 <- combi$FamilyID
combi$FamilyID2 <- as.character(combi$FamilyID2)
combi$FamilyID2[combi$FamilySize <= 3] <- 'Small'
combi$FamilyID2 <- factor(combi$FamilyID2)
# Re-splitting new training and test data
train <- combi[1:891, ]
test <- combi[892:1309, ]
# Loading the randomForest package
# install.packages('randomForest')
library(randomForest)
# Resetting the seed
set.seed(415)
# Fitting a random forest
fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID2, data=train, importance=TRUE, ntree= 2000)
# Analysing the most important features of the random forest
varImpPlot(fit)
# Generating Kaggle prediction file
Prediction <- predict(fit, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "Submissions/firstforest.csv", row.names = FALSE)
# Installing and loading the party package
# install.packages('party')
library(party)
# Setting the seed for consistent results and build a model in a similar way to random forests
set.seed(415)
fit <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID,
data = train, controls = cforest_unbiased(ntree=2000, mtry=3))
# Generating a prediction
Prediction <- predict(fit, test, OOB=TRUE, type = "response")
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "Submissions/cforest.csv", row.names = FALSE)
